<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Perception &mdash; Apollo Auto alpha documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/cyber/doxy-docs/source/main_stylesheet.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/tabs.js"></script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script>mermaid.initialize({startOnLoad:true});</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Perception" href="perception_apollo_3.0.html" />
    <link rel="prev" title="Linters and Formatters used in Apollo 6.0" href="linters_and_formatters.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: skyblue" >
            <a href="../../index.html" class="icon icon-home"> Apollo Auto
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../README.html">Apollo Auto 自述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../README.html">Apollo Auto 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cyber/doxy-docs/source/index.html">cyber 文档</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: skyblue" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Apollo Auto</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Perception</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/docs/specs/perception_apollo_2.5.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="perception">
<h1>Perception<a class="headerlink" href="#perception" title="Permalink to this headline"></a></h1>
<p>Apollo 2.5
April 19, 2018</p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline"></a></h2>
<p>Apollo 2.5 aims for Level-2 autonomous driving with low cost sensors. An autonomous vehicle will stay in the lane and keep a distance with a closest in-path vehicle (CIPV) using a single front-facing camera and a frontal radar. Apollo 2.5 supports high-speed autonomous driving on highway without any map. The deep network was learned to process an image data. The performance of the deep network will be improved over time as collecting more data.</p>
<p><em><strong>Safety alert</strong></em></p>
<p>Apollo 2.5 <em>does not</em> support a high curvature road, a road without lane marks including local roads and intersections. The perception module is based on the visual detection using a deep network with limited data. Therefor, before we release a better network, the driver should be careful in driving and always be ready to disengage the autonomous driving by turning the wheel to the right direction. Please perform the test drive at the safe and restricted area.</p>
<ul class="simple">
<li><p><em><strong>Recommended road</strong></em></p>
<ul>
<li><p><em><strong>Road with clear white lane lines on both sides</strong></em></p></li>
</ul>
</li>
<li><p><em><strong>Avoid</strong></em></p>
<ul>
<li><p><em><strong>High curvature road</strong></em></p></li>
<li><p><em><strong>Road without lane line marks</strong></em></p></li>
<li><p><em><strong>Intersection</strong></em></p></li>
<li><p><em><strong>Butt dots or dotted lane lines</strong></em></p></li>
<li><p><em><strong>Public road</strong></em></p></li>
</ul>
</li>
</ul>
</section>
<section id="perception-modules">
<h2>Perception modules<a class="headerlink" href="#perception-modules" title="Permalink to this headline"></a></h2>
<p>The flow chart of each module is shown below.</p>
<p><img alt="Image" src="../../_images/perception_flow_chart_apollo_2.5.png" /></p>
<p><strong>Figure 1: Flow diagram of lane keeping system</strong></p>
<section id="deep-network">
<h3>Deep network<a class="headerlink" href="#deep-network" title="Permalink to this headline"></a></h3>
<p>Deep network ingests an image and provides two detection outputs, lane lines and objects for Apollo 2.5. There is an ongoing debate on individual task and co-trained task for deep learning. Individual networks such as a lane detection network or an object detection network usually perform better than one co-trained multi-task network. However, with given limited resources, multiple individual networks will be costly and consume more time in processing. Therefore, for the economic design, co-train is inevitable with some compromise in performance. In Apollo 2.5, YOLO [1][2] was used as a base network of object and lane detection. The object has vehicle, truck, cyclist, and pedestrian categories and represented by a 2-D bounding box with orientation information. The lane lines are detected by segmentation using the same network with some modification.</p>
</section>
<section id="network-optimization">
<h3>Network optimization<a class="headerlink" href="#network-optimization" title="Permalink to this headline"></a></h3>
<p>In literature, there are multiple approaches of network optimization for real time processing of high framerate images. Rather than using 32bit float, a network with INT8 was implemented to achieve real-time implementation. TensorRT may be used to optimize the network.</p>
</section>
<section id="object-detection-tracking">
<h3>Object detection/tracking<a class="headerlink" href="#object-detection-tracking" title="Permalink to this headline"></a></h3>
<p>In a traffic scene, there are two kinds of objects: stationary objects and dynamic objects. Stationary objects include lane lines, traffic lights, and thousands of traffic signs written in different languages. Other than driving, there are multiple landmarks on the road mostly for visual localization including streetlamp, barrier, bridge on top of the road, or any skyline. For stationary object, we will detect only lane lines in Apollo 2.5.</p>
<p>Among dynamic objects, we care passenger vehicles, trucks, cyclists, pedestrians, or any other object including animal or body parts on the road. We can also categorize object based on which lane the object is in. The most important object is CIPV (closest object in our path). Next important objects would be the one in neighbor lanes.</p>
<section id="d-to-3d-bounding-box">
<h4>2D-to-3D bounding box<a class="headerlink" href="#d-to-3d-bounding-box" title="Permalink to this headline"></a></h4>
<p>Given a 2D box, with its 3D size and orientation in camera, this module searches the 3D position in a camera coordinate system and estimates an accurate 3D distance using either the width, the height, or the 2D area of that 2D box. The module works without accurate extrinsic camera parameters.</p>
</section>
<section id="object-tracking">
<h4>Object tracking<a class="headerlink" href="#object-tracking" title="Permalink to this headline"></a></h4>
<p>The object tracking module utilizes multiple cues such as 3D position, 2D image patches, 2D boxes, or deep learning ROI features. The tracking problem is formulated as multiple hypothesis data association by combining the cues efficiently to provide the most correct association between tracks and detected object, thus obtaining correct ID association for each object.</p>
</section>
</section>
<section id="lane-detection-tracking">
<h3>Lane detection/tracking<a class="headerlink" href="#lane-detection-tracking" title="Permalink to this headline"></a></h3>
<p>Among static objects, we will handle lane lines only in Apollo 2.5. The lane is for both longitudinal and lateral control. A lane itself guides lateral control and an object in the lane guides longitudinal control.</p>
<section id="lane-lines">
<h4>Lane lines<a class="headerlink" href="#lane-lines" title="Permalink to this headline"></a></h4>
<p>The lane can be represented by multiple sets of polylines such as next left lane line, left line, right line, and next right line. Given a heatmap of lane lines from the deep network, the segmented binary image is generated by thresholding. The method first finds the connected components and detects the inner contours. Then it generates lane marker points based on the contour edges in the ground space of ego-vehicle coordinate system. After that, it associates these lane markers into several lane line objects with corresponding relative spatial (e.g., left(L0), right(R0), next left(L1), next right(L2), etc.) labels.</p>
</section>
</section>
<section id="cipv-closest-in-path-vehicle">
<h3>CIPV (Closest-In Path Vehicle)<a class="headerlink" href="#cipv-closest-in-path-vehicle" title="Permalink to this headline"></a></h3>
<p>A CIPV is the closest vehicle in our ego-lane. An object is represented by 3D bounding box and its 2D projection from the top-down view localizes the object on the ground. Then, each object will be checked if it is in the ego-lane or not. Among the objects in our ego-lane, the closest one will be selected as a CIPV.</p>
</section>
<section id="radar-camera-fusion">
<h3>Radar + camera fusion<a class="headerlink" href="#radar-camera-fusion" title="Permalink to this headline"></a></h3>
<p>Given multiple sensors, their output should be combined in a synergic fashion. Apollo 2.5. introduces a sensor set with a radar and a camera. For this process, both sensors need to be calibrated. Each sensor will be calibrated using the same method introduced in Apollo 2.0. After calibration, the output will be represented in a 3-D world coordinate and each output will be fused by their similarity in location, size, time and the utility of each sensor. After learning the utility function of each sensor, the camera contributes more on lateral distance and the radar contributes more on longitudinal distance measurement.</p>
</section>
<section id="virtual-lane">
<h3>Virtual lane<a class="headerlink" href="#virtual-lane" title="Permalink to this headline"></a></h3>
<p>All lane detection results will be combined spatially and temporarily to induce the virtual lane which will be fed to planning and control. Some lane lines would be incorrect or missing in a certain frame. To provide the smooth lane line output, the history of lane lines using vehicle odometry is used. As the vehicle moves, the odometer of each frame is saved and lane lines in previous frames will be also saved in the history buffer. The detected lane line which does not match with the history lane lines will be removed and the history output will replace the lane line and be provided to the planning module.</p>
</section>
</section>
<section id="output-of-perception">
<h2>Output of perception<a class="headerlink" href="#output-of-perception" title="Permalink to this headline"></a></h2>
<p>The input of PnC will be quite different with that of the previous lidar-based system.</p>
<ul class="simple">
<li><p>Lane line output</p>
<ul>
<li><p>Polyline and/or a polynomial curve</p></li>
<li><p>Lane type by position: L1(next left lane line), L0(left lane line), R0(right lane line), R1(next right lane line)</p></li>
</ul>
</li>
<li><p>Object output</p>
<ul>
<li><p>3D rectangular cuboid</p></li>
<li><p>Relative velocity and direction</p></li>
<li><p>Type: CIPV, PIHP, others</p></li>
<li><p>Classification type: car, truck, bike, pedestrian</p></li>
</ul>
</li>
</ul>
<p>The world coordinate will be ego-coordinate in 3D where the rear center axle is an origin.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline"></a></h2>
<p>[1] J Redmon, S Divvala, R Girshick, A Farhadi, “You only look once: Unified, real-time object detection,” CVPR 2016</p>
<p>[2] J Redmon, A Farhadi, “YOLO9000: Better, Faster, Stronger,” arXiv preprint</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="linters_and_formatters.html" class="btn btn-neutral float-left" title="Linters and Formatters used in Apollo 6.0" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="perception_apollo_3.0.html" class="btn btn-neutral float-right" title="Perception" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, xinetzone.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>