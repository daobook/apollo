<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Traffic Light Perception &mdash; Apollo Auto alpha documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/cyber/doxy-docs/source/main_stylesheet.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/tabs.js"></script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script>mermaid.initialize({startOnLoad:true});</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="交通信号灯感知" href="traffic_light_cn.html" />
    <link rel="prev" title="参考线平滑设定" href="reference_line_smoother_cn.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: skyblue" >
            <a href="../../index.html" class="icon icon-home"> Apollo Auto
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../README.html">Apollo Auto 自述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../README.html">Apollo Auto 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cyber/doxy-docs/source/index.html">cyber 文档</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: skyblue" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Apollo Auto</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Traffic Light Perception</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/docs/specs/traffic_light.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="traffic-light-perception">
<h1>Traffic Light Perception<a class="headerlink" href="#traffic-light-perception" title="Permalink to this headline"></a></h1>
<p>This document provides the details about how traffic light perception functions in Apollo 2.0.</p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline"></a></h2>
<p>The Traffic Light Perception Module is designed to provide accurate and comprehensive traffic light status using cameras.</p>
<p>Typically, the traffic light has three states:</p>
<ul class="simple">
<li><p>Red</p></li>
<li><p>Yellow</p></li>
<li><p>Green</p></li>
</ul>
<p>However, if the traffic light is not working, it might display the color black or show a flashing red or yellow light. Sometimes the traffic light cannot be found in the camera’s field of vision and the module fails to recognize its status.</p>
<p>To account for all situations, the Traffic Light Perception Module provides output for five states:</p>
<ul class="simple">
<li><p>Red</p></li>
<li><p>Yellow</p></li>
<li><p>Green</p></li>
<li><p>Black</p></li>
<li><p>Unknown</p></li>
</ul>
<p>The module’s HD-Map queries repeatedly to know whether there are lights present in front of the vehicle. The traffic light is represented by the four points on its boundary, which can be obtained by querying the HD-Map, given the car’s location. The traffic light is projected from world coordinates to image coordinates if there is a light in front of the vehicle.</p>
<p>Apollo has determined that using a single camera, which has a constant field of vision, cannot see traffic lights everywhere. This limitation is due to the following factors:</p>
<ul class="simple">
<li><p>The perception range should be above 100 meters</p></li>
<li><p>The height of the traffic lights or the width of crossing varies widely</p></li>
</ul>
<p>Consequently, Apollo 2.0 uses two cameras to enlarge its perception range:</p>
<ul class="simple">
<li><p>A <strong>telephoto</strong> <strong>camera</strong>, whose focus length is 25 mm, is installed to observe forward, distant traffic lights. Traffic lights that are captured in a telephoto camera are very large and easy to detect. However, the field of vision of a telephoto camera is quite limited. The lights are often outside of the image if the lane is not straight enough, or if the car is in very close proximity to the light.</p></li>
<li><p>A <strong>wide-angle camera</strong>, whose focus length is 6 mm, is equipped to provide a supplementary field of vision.</p></li>
</ul>
<p>The module decides which camera to use adaptively based on the light projection. Although there are only two cameras on the Apollo car, the algorithm can handle multiple cameras.</p>
<p>The following photos show the detection of traffic lights using a telephoto camera (for the first photo) and a wide-angle camera (for the second photo).</p>
<p><img alt="telephoto camera" src="../../_images/long.jpg" /></p>
<p><img alt="wide angle camera" src="../../_images/short.jpg" /></p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="pipeline">
<h1>Pipeline<a class="headerlink" href="#pipeline" title="Permalink to this headline"></a></h1>
<p>The Pipeline has two main parts and is described in following sections:</p>
<ul class="simple">
<li><p>Pre-process</p>
<ul>
<li><p>Traffic light projection</p></li>
<li><p>Camera selection</p></li>
<li><p>Image and cached lights sync</p></li>
</ul>
</li>
<li><p>Process</p>
<ul>
<li><p>Rectify — Provide the accurate traffic light bounding boxes</p></li>
<li><p>Recognize — Provide the color of each bounding box</p></li>
<li><p>Revise — Correct the color based on the time sequence</p></li>
</ul>
</li>
</ul>
<section id="pre-process">
<h2>Pre-process<a class="headerlink" href="#pre-process" title="Permalink to this headline"></a></h2>
<p>There is no need to detect lights in every frame of an image. The status of a traffic light changes in low frequency and the computing resources are limited. Normally, images from different cameras would arrive almost simultaneously, and only one is fed to the Process part of the Pipeline. Therefore, the selection and the matching of images are necessary.</p>
<section id="input-output">
<h3>Input/Output<a class="headerlink" href="#input-output" title="Permalink to this headline"></a></h3>
<p>This section describes the input and the output of the Pre-process module. The input is obtained by subscribing to topic names from Apollo or directly reading them from locally stored files, and the output is fed to the successive Process module.</p>
<section id="input">
<h4>Input<a class="headerlink" href="#input" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><p>Images from different cameras, acquired by subscribing to the topic name:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">/apollo/sensor/camera/traffic/image_long</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/apollo/sensor/camera/traffic/image_short</span></code></p></li>
</ul>
</li>
<li><p>Localization, acquired by querying the topic:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">/tf</span></code></p></li>
</ul>
</li>
<li><p>HD Map</p></li>
<li><p>Calibration results</p></li>
</ul>
</section>
<section id="output">
<h4>Output<a class="headerlink" href="#output" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><p>Image from the selected camera</p></li>
<li><p>Traffic light bounding box projected from world coordinates to image coordinates</p></li>
</ul>
</section>
</section>
<section id="camera-selection">
<h3>Camera Selection<a class="headerlink" href="#camera-selection" title="Permalink to this headline"></a></h3>
<p>The traffic light is represented by a unique ID and four points on its boundary, each of which is described as a 3D point in the world coordinate system.</p>
<p>The following example shows a typical entry for traffic light <code class="docutils literal notranslate"><span class="pre">signal</span> <span class="pre">info</span></code>. The four boundary points can be obtained by querying the HD Map, given the car’s location.</p>
<div class="highlight-protobuf notranslate"><div class="highlight"><pre><span></span><span class="n">signal</span> <span class="n">info</span><span class="o">:</span>
<span class="n">id</span> <span class="p">{</span>
  <span class="n">id</span><span class="o">:</span> <span class="s">&quot;xxx&quot;</span>
<span class="p">}</span>
<span class="n">boundary</span> <span class="p">{</span>
  <span class="n">point</span> <span class="p">{</span> <span class="n">x</span><span class="o">:</span> <span class="o">...</span>  <span class="n">y</span><span class="o">:</span> <span class="o">...</span>  <span class="n">z</span><span class="o">:</span> <span class="o">...</span>  <span class="p">}</span>
  <span class="n">point</span> <span class="p">{</span> <span class="n">x</span><span class="o">:</span> <span class="o">...</span>  <span class="n">y</span><span class="o">:</span> <span class="o">...</span>  <span class="n">z</span><span class="o">:</span> <span class="o">...</span>  <span class="p">}</span>
  <span class="n">point</span> <span class="p">{</span> <span class="n">x</span><span class="o">:</span> <span class="o">...</span>  <span class="n">y</span><span class="o">:</span> <span class="o">...</span>  <span class="n">z</span><span class="o">:</span> <span class="o">...</span>  <span class="p">}</span>
  <span class="n">point</span> <span class="p">{</span> <span class="n">x</span><span class="o">:</span> <span class="o">...</span>  <span class="n">y</span><span class="o">:</span> <span class="o">...</span>  <span class="n">z</span><span class="o">:</span> <span class="o">...</span>  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The boundary points in the 3D world coordinates are then projected to the 2D image coordinates of each camera. For one traffic light, the bounding box described by the four projected points in the telephoto camera image has a larger area. It is better for detection than that in the wide-range image. Consequently,  the image from the camera with the longest focal length that can see all the lights will be selected as the output image. The traffic light bounding box projected on this image will be the output bounding box.</p>
<p>The selected camera ID with timestamp is cached in queue, as shown below:</p>
<div class="highlight-C++ notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span> <span class="nc">ImageLights</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w"> </span><span class="n">CarPose</span><span class="w"> </span><span class="n">pose</span><span class="p">;</span><span class="w"></span>
<span class="w"> </span><span class="n">CameraId</span><span class="w"> </span><span class="n">camera_id</span><span class="p">;</span><span class="w"></span>
<span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">timestamp</span><span class="p">;</span><span class="w"></span>
<span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">num_signal</span><span class="p">;</span><span class="w"></span>
<span class="w"> </span><span class="p">...</span><span class="w"> </span><span class="n">other</span><span class="w"> </span><span class="p">...</span><span class="w"></span>
<span class="p">};</span><span class="w"></span>
</pre></div>
</div>
<p>Thus far, all the information that we need includes the localization, the calibration results, and the HD Map. The selection can be performed at any time as the projection is independent of the image content. The task of performing the selection when images arrive is just for simplicity. Moreover, image selection does not need to be performed upon the arrival of every image, and a time interval for the selection is set.</p>
</section>
<section id="image-sync">
<h3>Image Sync<a class="headerlink" href="#image-sync" title="Permalink to this headline"></a></h3>
<p>Images arrive with a timestamp and a camera ID. The pairing of a timestamp and a camera ID is used to find the appropriate cached information. If the image can find a cached record with same camera ID and a small difference between timestamps, the image can be published to the Process module. All inappropriate images are abandoned.</p>
</section>
</section>
<section id="process">
<h2>Process<a class="headerlink" href="#process" title="Permalink to this headline"></a></h2>
<p>The Process module is divided into three steps, with each step focusing on one task:</p>
<ul class="simple">
<li><p>Rectifier — Detects a traffic light bounding box in a ROI.</p></li>
<li><p>Recognizer— Classifies the bounding box’s color.</p></li>
<li><p>Reviser — Correct color using sequential information.</p></li>
</ul>
<section id="id1">
<h3>Input/Output<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h3>
<p>This section describes the data input and output of the Process. The input is obtained from the Pre-process module and the output is published as a traffic light topic.</p>
<section id="id2">
<h4>Input<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><p>Image from a selected camera</p></li>
<li><p>A set of bounding boxes</p></li>
</ul>
</section>
<section id="id3">
<h4>Output<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><p>A set of bounding boxes with color labels.</p></li>
</ul>
</section>
</section>
<section id="rectifier">
<h3>Rectifier<a class="headerlink" href="#rectifier" title="Permalink to this headline"></a></h3>
<p>The projected position, which is affected by the calibration, localization, and the HD-Map label, is <em><strong>not completely reliable</strong></em>. A larger region of interest (ROI), calculated using the projected light’s position, is used to find the accurate boundingbox for the traffic light.</p>
<p>In the photo below, the blue rectangle indicates the projected light bounding box, which has a large offset to the actual light. The big, yellow rectangle is the ROI.</p>
<p><img alt="example" src="../../_images/example.jpg" /></p>
<p>The traffic light detection is implemented as a regular convolutional neural network (CNN) detection task. It receives an image with an ROI as input, and serial bounding boxes as output. There might be more lights in the ROI than those in input.</p>
<p>Apollo needs to select the proper lights according to the detection score, and the input lights’ position and shape. If the CNN network cannot find any lights in the ROI, the status from the input lights is marked as unknown and the two remaining steps (Recognizer and Reviser) are skipped.</p>
</section>
<section id="recognizer">
<h3>Recognizer<a class="headerlink" href="#recognizer" title="Permalink to this headline"></a></h3>
<p>The traffic light recognition is implemented as a typical CNN classification task. The network receives an image with a ROI and a list of bounding boxes as input. The output of network is a <code class="docutils literal notranslate"><span class="pre">$4\times</span> <span class="pre">n$</span> <span class="pre">vector</span></code>, representing four probabilities for each box to be black, red, yellow, and green.</p>
<p>The class with maximum probability will be regarded as the light’s status, if and only if the probability is large enough. Otherwise, the light’s status will be set to black, which means that the status is not certain.</p>
</section>
<section id="reviser">
<h3>Reviser<a class="headerlink" href="#reviser" title="Permalink to this headline"></a></h3>
<p>Because a traffic light can be flashing or shaded, and the Recognizer is <em><strong>not</strong></em> perfect, the current status may fail to represent the real status. A Reviser that could correct the status is necessary.</p>
<p>If the Reviser receives a definitive status such as red or green, the Reviser saves and outputs the status directly. If the received status is black or unknown, the Reviser looks up the saved map. When the status of this light is certain for a period of time, the Reviser outputs this saved status. Otherwise, the status of black or unknown is sent as output.</p>
<p>Because of the time sequence, yellow only exists <em><strong>after</strong></em> green and <em><strong>before</strong></em> red. Any yellow <em><strong>after red</strong></em> is reset to red for the sake of safety until green displays.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="reference_line_smoother_cn.html" class="btn btn-neutral float-left" title="参考线平滑设定" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="traffic_light_cn.html" class="btn btn-neutral float-right" title="交通信号灯感知" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, xinetzone.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>