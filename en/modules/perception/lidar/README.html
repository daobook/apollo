<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Lidar Perception &mdash; Apollo Auto alpha documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/cyber/doxy-docs/source/main_stylesheet.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/tabs.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
        <script>mermaid.initialize({startOnLoad:true});</script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Lidar Perception" href="README_6.0.html" />
    <link rel="prev" title="2018-09-02" href="../camera/tools/offline/data/perception/camera/models/yolo_obstacle_detector/3d-r4-half/CHANGELOG.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: skyblue" >
            <a href="../../../index.html" class="icon icon-home"> Apollo Auto
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../README.html">Apollo Auto 自述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docs/README.html">Apollo Auto 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cyber/doxy-docs/source/index.html">cyber 文档</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: skyblue" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Apollo Auto</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Lidar Perception</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/modules/perception/lidar/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="lidar-perception">
<h1>Lidar Perception<a class="headerlink" href="#lidar-perception" title="Permalink to this headline"></a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline"></a></h2>
<p>In Apollo 7.0, a new LiDAR-based obstacle detection model is provided named Mask-Pillars based on PointPillars, which improves the original version in two aspects. The first one is that a residual attention module is introduced into the encoder of the backbone to learn a mask and to enhance the feature map in a residual way. The second one is that a pillar-level supervision is applied after decoder of the backbone which is only performed in the training stage. The training data for pillar-level supervision is generated by composing the distribution of foreground obstacle pillars. From the experimental validation, Mask-Pillars achieves higher performance than PointPillars on both Kitti and Waymo datasets, especially the recall on obstacles.</p>
</section>
<section id="architecture">
<h2>Architecture<a class="headerlink" href="#architecture" title="Permalink to this headline"></a></h2>
<p>Here we mainly focus on the modifications based on PointPillars:</p>
<section id="attention-module">
<h3>Attention Module<a class="headerlink" href="#attention-module" title="Permalink to this headline"></a></h3>
<p>Although LiDAR can collect high-quality point cloud data, some obstacles may have a small number of point clouds due to occlusion or distance. Therefore, we introduce an attention layer on FPN encoder module to enhance the features refer to <a class="reference external" href="https://arxiv.org/abs/1704.06904">Residual Attention Network for Image Classification</a>. Since FPN has three feature maps with different resolutions, our attention module also acts on three feature maps at the same time. More details about the network architecture can refer to figure below，S represent Sigmoid，F function is shown as Formula 1：</p>
<div class="math notranslate nohighlight">
\[
F(x) = (1 + M(x)) * T(x)
\tag{1}
\]</div>
<p><span class="math notranslate nohighlight">\(T(x)\)</span>is the output of backbone，<span class="math notranslate nohighlight">\(M(x)\)</span>is the output of attention module.</p>
</section>
<section id="pillar-level-supervision">
<h3>Pillar-level supervision<a class="headerlink" href="#pillar-level-supervision" title="Permalink to this headline"></a></h3>
<p>In order to improve the recall of the network, we introduce a pillar-level supervision mechanism in the training stage. We notice that the segmentation algorithms always have high recall rates because of the pixel level supervision. Therefore, we borrow the idea of segmentation network by adding a pillar-level supervision of foreground pillars that representing obstacles. The feature maps before feeding into the detection module are supervised by the pillar supervision data, which are represented as obstacle distributions. The supervision data are simply generated by composing the Guassion distributions of obstacle pillars of the training point cloud.</p>
<p>The network structure of the final FPN is shown in the figure below</p>
<div align=center>
<img src="../../../docs/specs/images/3d_obstacle_perception/lidar_network.png" alt="图片名称" width="60%" />
</div>
</section>
</section>
<section id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this headline"></a></h2>
<p>We apply the MMDetection3D framework for training. On the KITTI validation set, the results are as shown in the below table. The results of PointPillars comes from <a class="reference external" href="https://github.com/open-mmlab/mmdetection3d/blob/master/configs/pointpillars/README.md">mmdetction3d</a></p>
<div align=center>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="text-align:center head"><p>3DmAP <br> Mod.</p></th>
<th class="text-align:center head"><p>Car <br> Easy Mod. Hard</p></th>
<th class="text-align:center head"><p>Pedestrian <br> Easy Mod. Hard</p></th>
<th class="text-align:center head"><p>Cyclist <br> Easy Mod. Hard</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>PointPillars</p></td>
<td class="text-align:center"><p>60.11</p></td>
<td class="text-align:center"><p>85.41     73.98	 67.76</p></td>
<td class="text-align:center"><p>52.02	      46.40        42.48</p></td>
<td class="text-align:center"><p>78.72	   59.95	57.25</p></td>
</tr>
<tr class="row-odd"><td><p>Ours</p></td>
<td class="text-align:center"><p>62.07</p></td>
<td class="text-align:center"><p>86.13     76.74	 74.14</p></td>
<td class="text-align:center"><p>50.79	      45.59	       41.50</p></td>
<td class="text-align:center"><p>83.91	   63.87	61.05</p></td>
</tr>
</tbody>
</table>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="text-align:center head"><p>BEVmAP <br> Mod.</p></th>
<th class="text-align:center head"><p>Car <br> Easy Mod. Hard</p></th>
<th class="text-align:center head"><p>Pedestrian <br> Easy Mod. Hard</p></th>
<th class="text-align:center head"><p>Cyclist <br> Easy Mod. Hard</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>PointPillars</p></td>
<td class="text-align:center"><p>67.76</p></td>
<td class="text-align:center"><p>89.93     86.57	 85.20</p></td>
<td class="text-align:center"><p>59.08	      53.36	       48.42</p></td>
<td class="text-align:center"><p>80.93	   63.34	60.06</p></td>
</tr>
<tr class="row-odd"><td><p>Ours</p></td>
<td class="text-align:center"><p>69.49</p></td>
<td class="text-align:center"><p>89.85     87.15	 85.55</p></td>
<td class="text-align:center"><p>58.29	      53.87	       49.98</p></td>
<td class="text-align:center"><p>85.13	   67.43	63.85</p></td>
</tr>
</tbody>
</table>
</div>
<p>The detection visualization on KITTI data of PointPillars and our model are shown as below. It can be seen that our model has better detection performance. We can see that truncated and occluded vehicles are recalled by our model.</p>
<div align=center>
<img src="../../../docs/specs/images/3d_obstacle_perception/lidar_detection_compare.png" alt="图片名称" width="60%" />
</div>
</section>
<section id="online">
<h2>Online<a class="headerlink" href="#online" title="Permalink to this headline"></a></h2>
<p>Here, we use libtorch for online deployment and use the torch.jit.trace function of pytorch. We divide the original model into five parts. For more details, please refer to the code：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;modules/perception/lidar/lib/detector/point_pillars_detection/point_pillars.cc&quot;</span>
</pre></div>
</div>
</section>
<section id="launch">
<h2>Launch<a class="headerlink" href="#launch" title="Permalink to this headline"></a></h2>
<p>In order to facilitate the extension of Apollo models, we refactor the detection module that allows more pluggable detection models. To choose a specific model, you only need to modify corresponding configuration files. The configuration files are referred to:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">modules</span><span class="o">/</span><span class="n">perception</span><span class="o">/</span><span class="n">production</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">perception</span><span class="o">/</span><span class="n">lidar</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">lidar_obstacle_pipeline</span><span class="o">/</span>
</pre></div>
</div>
<p>There are several directories in the path, which are related to corresponding sensors. For LiDAR sensor, you could modify the value of key “detector” in “lidar_obstacle_detection.conf” file to switch the LiDAR-based detection model.</p>
</section>
<section id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>MMDetection3D: OpenMMLab next-generation platform for general 3D object detection <a class="reference external" href="https://github.com/open-mmlab/mmdetection3d">https://github.com/open-mmlab/mmdetection3d</a></p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../camera/tools/offline/data/perception/camera/models/yolo_obstacle_detector/3d-r4-half/CHANGELOG.html" class="btn btn-neutral float-left" title="2018-09-02" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="README_6.0.html" class="btn btn-neutral float-right" title="Lidar Perception" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, xinetzone.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>